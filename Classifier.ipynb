{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class constructor or initialization method.\n",
    "        '''\n",
    "    def fit(self, algorithm='SVM', c = 1.0, kernel='rbf'):\n",
    "        v = DictVectorizer()\n",
    "        self.X = v.fit_transform(self.x)\n",
    "        if algorithm == 'SVM':\n",
    "            self.clf = svm.SVC(C=c,kernel=kernel)\n",
    "            self.clf.fit(self.X,self.y)\n",
    "        elif algorithm == 'NN':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self.X, self.y)\n",
    "            #scaler = StandardScaler()\n",
    "            #scaler.fit(X_train)\n",
    "            #X_train = scaler.transform(X_train)\n",
    "            #X_test = scaler.transform(X_test)\n",
    "            mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "            mlp.fit(X_train,y_train)\n",
    "            predictions = mlp.predict(X_test)\n",
    "            print(confusion_matrix(y_test,predictions))\n",
    "            print(classification_report(y_test,predictions))\n",
    "            \n",
    "    \n",
    "    def predict(self):\n",
    "        self.clf.fit(self.X, self.y)\n",
    "        for x, y in zip(self.X,self.y):\n",
    "            print(\"{} {}\".format(self.clf.predict(x),y))\n",
    "    \n",
    "    def validate(self, splits=10, test=0.1):\n",
    "        cv = ShuffleSplit(n_splits=splits, test_size=test)#, random_state=0)\n",
    "        scores = cross_val_score(self.clf, self.X, self.y, cv=cv)\n",
    "        return scores.mean()\n",
    "        \n",
    "    def train_score(self):\n",
    "        return self.clf.score(self.X,self.y)\n",
    "        \n",
    "    def load_data(self, tweets, prices, threshold=0):\n",
    "        # Check market change\n",
    "        prices['grew'] = np.select([prices['close']>prices['open']*(1+threshold),\n",
    "                             prices['close']<prices['open']*(1-threshold)],[1,-1],default=0)\n",
    "        \n",
    "        (prices['close']-prices['open']) > 0\n",
    "        hours = prices['time']\n",
    "        \n",
    "        self.x ={}\n",
    "        self.y ={}\n",
    "        for hour in hours:\n",
    "            self.x[hour] = defaultdict(int)\n",
    "            self.y[hour] = price_df.loc[price_df['time']==hour].iloc[0]['grew']\n",
    "            \n",
    "        wnl = WordNetLemmatizer()\n",
    "        stopset = stopwords.words('english')\n",
    "        ignore = str.maketrans('', '', string.punctuation+string.digits)\n",
    "        for index, row in tweets_df.iterrows():\n",
    "            tag = row['rounded_dateTime']\n",
    "            words = set()\n",
    "            line = row['text'].translate(ignore)\n",
    "            tokens = word_tokenize(line)\n",
    "            tokens = [ wnl.lemmatize(token.lower()) for token in tokens ]\n",
    "            tokens = [w for w in tokens if not w in stopset]\n",
    "            bigrm = bigrams(tokens)\n",
    "            for word in tokens:\n",
    "                words.add(word)\n",
    "            for word in words:\n",
    "                self.x[tag][word] += 1\n",
    "            for gram in bigrm:\n",
    "                word = gram[0]+gram[1]\n",
    "                self.x[tag][word] += 1\n",
    "                \n",
    "        self.x = [self.x[key] for key in sorted(self.x.keys())]\n",
    "        self.y = [self.y[key] for key in sorted(self.y.keys())]\n",
    "    \n",
    "    def trim(self):\n",
    "        new_x = []\n",
    "        new_y = []\n",
    "        for a, b in zip(self.x, self.y):\n",
    "            if len(a) > 0:\n",
    "                new_x.append(a)\n",
    "                new_y.append(b)\n",
    "        self.x=new_x\n",
    "        self.y=new_y\n",
    "    def shift(self, shift):\n",
    "        self.x = self.x[:len(self.x)-shift]\n",
    "        self.y = self.y[shift:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_pickle('Data/Cleaned/tweets_prices.pkl')\n",
    "price_df = pd.read_pickle('Data/Cleaned/xrp_price_data_hourly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier()\n",
    "classifier.load_data(tweets_df, price_df, threshold=0.00)\n",
    "classifier.trim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  6]\n",
      " [29 13]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.40      0.76      0.52        25\n",
      "          1       0.68      0.31      0.43        42\n",
      "\n",
      "avg / total       0.58      0.48      0.46        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(algorithm='NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599250936329588\n",
      "0.5462962962962963\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(c=2.0)\n",
    "print(classifier.train_score())\n",
    "#classifier.predict()\n",
    "print(classifier.validate(splits=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
