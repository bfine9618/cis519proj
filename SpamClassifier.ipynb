{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 Part I: Spam Classification in SciKit-Learn\n",
    "\n",
    "This assignment uses data from https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "Data processing was inspired by https://www.kaggle.com/overflow012/d/uciml/sms-spam-collection-dataset/text-preprocessing-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, run this to upgrade SciKit-Learn to 0.19.1.  Then go to Kernel | Restart in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urlmarker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dbc870f6adc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mregularize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregularize_urls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularize_numbers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\Junior\\CIS 519\\Project\\cis519proj\\regularize.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murlmarker\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Call with dataframe column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mregularize_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWEB_URL_REGEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' _url_ '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urlmarker'"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from regularize import regularize_urls, regularize_numbers\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "old comet version (1.0.3) detected. current: 1.0.5 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "Comet.ml support for Ipython Notebook is limited at the moment, automatic monitoring and stdout capturing is deactivated\n",
      "\n",
      "Experiment is live on comet.ml https://www.comet.ml/bfine9618/general/3a0568e094ac4710a606e0e0819190cb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment  = Experiment(api_key=\"74yk7vDQYudlokIQriZfS10ej\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Helper function:\n",
    "#  Return the k most frequently appearing keywords in the dataframe\n",
    "def top_k(data_df, vec, k):\n",
    "    X = vec.fit_transform(data_df['sms'].values)\n",
    "    labels = vec.get_feature_names()\n",
    "    \n",
    "    return pd.DataFrame(columns = labels, data = X.toarray()).sum().sort_values(ascending = False)[:k]\n",
    "\n",
    "\n",
    "\n",
    "sms_df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "sms_df.columns = ['class', 'sms', 'a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1 Data Wrangling\n",
    "\n",
    "Clean up sms_df.  Delete 'a', 'b', 'c', lowercase the sms text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Data wrangling / cleaning\n",
    "sms_df.drop(['a', 'b', 'c'], axis=1, inplace=True)\n",
    "sms_df['sms'] = sms_df['sms'].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>freemsg hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>as per your request 'melle melle (oru minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>winner!! as a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>had your mobile 11 months or more? u r entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>i'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>six chances to win cash! from 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>urgent! you have won a 1 week free membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>i've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>i have a date on sunday with will!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>xxxmobilemovieclub: to use your credit, click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>oh k...i'm watching here:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>eh u remember how 2 spell his name... yes i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>fine if thatåõs the way u feel. thatåõs the wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>england v macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>is that seriously how you spell his name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>iû÷m going to try for 2 months ha ha only joking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ham</td>\n",
       "      <td>so ì_ pay first lar... then when is da stock c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ham</td>\n",
       "      <td>aft i finish my lunch then i go str down lor. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>ffffffffff. alright no way i can meet up with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ham</td>\n",
       "      <td>just forced myself to eat a slice. i'm really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ham</td>\n",
       "      <td>lol your always so convincing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ham</td>\n",
       "      <td>did you catch the bus ? are you frying an egg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ham</td>\n",
       "      <td>i'm back &amp;amp; we're packing the car now, i'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ham</td>\n",
       "      <td>ahhh. work. i vaguely remember that! what does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>ham</td>\n",
       "      <td>armand says get your ass over to epsilon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>ham</td>\n",
       "      <td>u still havent got urself a jacket ah?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>ham</td>\n",
       "      <td>i'm taking derek &amp;amp; taylor to walmart, if i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>ham</td>\n",
       "      <td>hi its in durban are you still on this number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>ham</td>\n",
       "      <td>ic. there are a lotta childporn cars then.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>spam</td>\n",
       "      <td>had your contract mobile 11 mnths? latest moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>ham</td>\n",
       "      <td>no, i was trying it all weekend ;v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>ham</td>\n",
       "      <td>you know, wot people wear. t shirts, jumpers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>ham</td>\n",
       "      <td>cool, what time you think you can get here?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>ham</td>\n",
       "      <td>wen did you get so spiritual and deep. that's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>ham</td>\n",
       "      <td>have a safe trip to nigeria. wish you happines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5553</th>\n",
       "      <td>ham</td>\n",
       "      <td>hahaha..use your brain dear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>ham</td>\n",
       "      <td>well keep in mind i've only got enough gas for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>ham</td>\n",
       "      <td>yeh. indians was nice. tho it did kane me off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes i have. so that's why u texted. pshew...mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>ham</td>\n",
       "      <td>no. i meant the calculation is the same. that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry, i'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>ham</td>\n",
       "      <td>if you aren't here in the next  &amp;lt;#&amp;gt;  hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>ham</td>\n",
       "      <td>anything lor. juz both of us lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>ham</td>\n",
       "      <td>get me out of this dump heap. my mom decided t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lor... sony ericsson salesman... i ask shuh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>ham</td>\n",
       "      <td>ard 6 like dat lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>why don't you wait 'til at least wednesday to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>huh y lei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>reminder from o2: to get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>will ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>pity, * was in mood for that. so...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>the guy did some bitching but i acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl. its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                                sms\n",
       "0      ham  go until jurong point, crazy.. available only ...\n",
       "1      ham                      ok lar... joking wif u oni...\n",
       "2     spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3      ham  u dun say so early hor... u c already then say...\n",
       "4      ham  nah i don't think he goes to usf, he lives aro...\n",
       "5     spam  freemsg hey there darling it's been 3 week's n...\n",
       "6      ham  even my brother is not like to speak with me. ...\n",
       "7      ham  as per your request 'melle melle (oru minnamin...\n",
       "8     spam  winner!! as a valued network customer you have...\n",
       "9     spam  had your mobile 11 months or more? u r entitle...\n",
       "10     ham  i'm gonna be home soon and i don't want to tal...\n",
       "11    spam  six chances to win cash! from 100 to 20,000 po...\n",
       "12    spam  urgent! you have won a 1 week free membership ...\n",
       "13     ham  i've been searching for the right words to tha...\n",
       "14     ham                i have a date on sunday with will!!\n",
       "15    spam  xxxmobilemovieclub: to use your credit, click ...\n",
       "16     ham                         oh k...i'm watching here:)\n",
       "17     ham  eh u remember how 2 spell his name... yes i di...\n",
       "18     ham  fine if thatåõs the way u feel. thatåõs the wa...\n",
       "19    spam  england v macedonia - dont miss the goals/team...\n",
       "20     ham          is that seriously how you spell his name?\n",
       "21     ham  iû÷m going to try for 2 months ha ha only joking\n",
       "22     ham  so ì_ pay first lar... then when is da stock c...\n",
       "23     ham  aft i finish my lunch then i go str down lor. ...\n",
       "24     ham  ffffffffff. alright no way i can meet up with ...\n",
       "25     ham  just forced myself to eat a slice. i'm really ...\n",
       "26     ham                     lol your always so convincing.\n",
       "27     ham  did you catch the bus ? are you frying an egg ...\n",
       "28     ham  i'm back &amp; we're packing the car now, i'll...\n",
       "29     ham  ahhh. work. i vaguely remember that! what does...\n",
       "...    ...                                                ...\n",
       "5542   ham           armand says get your ass over to epsilon\n",
       "5543   ham             u still havent got urself a jacket ah?\n",
       "5544   ham  i'm taking derek &amp; taylor to walmart, if i...\n",
       "5545   ham      hi its in durban are you still on this number\n",
       "5546   ham         ic. there are a lotta childporn cars then.\n",
       "5547  spam  had your contract mobile 11 mnths? latest moto...\n",
       "5548   ham                 no, i was trying it all weekend ;v\n",
       "5549   ham  you know, wot people wear. t shirts, jumpers, ...\n",
       "5550   ham        cool, what time you think you can get here?\n",
       "5551   ham  wen did you get so spiritual and deep. that's ...\n",
       "5552   ham  have a safe trip to nigeria. wish you happines...\n",
       "5553   ham                        hahaha..use your brain dear\n",
       "5554   ham  well keep in mind i've only got enough gas for...\n",
       "5555   ham  yeh. indians was nice. tho it did kane me off ...\n",
       "5556   ham  yes i have. so that's why u texted. pshew...mi...\n",
       "5557   ham  no. i meant the calculation is the same. that ...\n",
       "5558   ham                             sorry, i'll call later\n",
       "5559   ham  if you aren't here in the next  &lt;#&gt;  hou...\n",
       "5560   ham                  anything lor. juz both of us lor.\n",
       "5561   ham  get me out of this dump heap. my mom decided t...\n",
       "5562   ham  ok lor... sony ericsson salesman... i ask shuh...\n",
       "5563   ham                                ard 6 like dat lor.\n",
       "5564   ham  why don't you wait 'til at least wednesday to ...\n",
       "5565   ham                                       huh y lei...\n",
       "5566  spam  reminder from o2: to get 2.50 pounds free call...\n",
       "5567  spam  this is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              will ì_ b going to esplanade fr home?\n",
       "5569   ham  pity, * was in mood for that. so...any other s...\n",
       "5570   ham  the guy did some bitching but i acted like i'd...\n",
       "5571   ham                         rofl. its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">sms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4515</td>\n",
       "      <td>sorry, i'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sms                                                               \n",
       "      count unique                                                top freq\n",
       "class                                                                     \n",
       "ham    4825   4515                             sorry, i'll call later   30\n",
       "spam    747    653  please call our customer service representativ...    4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.groupby('class').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2. Vectorizing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Generate feature vectors\n",
    "\n",
    "vec = CountVectorizer(decode_error = 'ignore', stop_words = 'english')\n",
    "X = vec.fit_transform(sms_df['sms'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the most frequent terms in spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "free          224\n",
       "txt           163\n",
       "ur            144\n",
       "mobile        127\n",
       "text          125\n",
       "stop          121\n",
       "claim         113\n",
       "reply         104\n",
       "www            98\n",
       "prize          93\n",
       "just           78\n",
       "cash           76\n",
       "won            76\n",
       "uk             74\n",
       "150p           71\n",
       "send           70\n",
       "new            69\n",
       "nokia          67\n",
       "win            64\n",
       "urgent         63\n",
       "tone           60\n",
       "week           60\n",
       "50             57\n",
       "contact        56\n",
       "service        56\n",
       "msg            54\n",
       "com            54\n",
       "18             51\n",
       "16             51\n",
       "guaranteed     50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_spam = top_k(sms_df[sms_df['class'] == 'spam'], vec, 30)\n",
    "\n",
    "top_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vs ham..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt       318\n",
       "lt       316\n",
       "just     293\n",
       "ok       287\n",
       "ll       265\n",
       "ur       241\n",
       "know     236\n",
       "good     233\n",
       "got      232\n",
       "like     232\n",
       "come     227\n",
       "day      209\n",
       "time     201\n",
       "love     199\n",
       "going    169\n",
       "home     165\n",
       "want     164\n",
       "lor      162\n",
       "need     158\n",
       "sorry    157\n",
       "don      151\n",
       "da       150\n",
       "today    139\n",
       "later    135\n",
       "dont     132\n",
       "did      129\n",
       "send     129\n",
       "think    128\n",
       "pls      123\n",
       "hi       122\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ham = top_k(sms_df[sms_df['class'] == 'ham'], vec, 30)\n",
    "\n",
    "top_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2.2 Regularize URLs and Numbers\n",
    "\n",
    "Import _regularize_ here, and use *regularize_urls* and *regularize_numbers*\n",
    "on the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Regularize/tokenize URLs and numbers\n",
    "sms_df['sms'] = regularize_urls(sms_df['sms'])\n",
    "sms_df['sms'] = regularize_numbers(sms_df['sms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2.2 Results\n",
    "\n",
    "Re-run the CountVectorizer, re-create vector X, and re-compute the top-30 spam terms.  Output the top-30 spam terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_num_         3289\n",
       "free           228\n",
       "txt            165\n",
       "ur             144\n",
       "_url_          141\n",
       "mobile         129\n",
       "stop           126\n",
       "text           125\n",
       "claim          113\n",
       "reply          104\n",
       "prize           92\n",
       "just            78\n",
       "won             76\n",
       "cash            76\n",
       "nokia           71\n",
       "send            70\n",
       "win             70\n",
       "new             69\n",
       "urgent          63\n",
       "week            60\n",
       "tone            59\n",
       "box             57\n",
       "msg             56\n",
       "service         56\n",
       "contact         56\n",
       "guaranteed      50\n",
       "ppm             49\n",
       "customer        49\n",
       "mins            47\n",
       "phone           46\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Top-30 spam terms\n",
    "vec = CountVectorizer(decode_error = 'ignore', stop_words = 'english')\n",
    "X = vec.fit_transform(sms_df['sms'].values)\n",
    "\n",
    "top_spam = top_k(sms_df[sms_df['class'] == 'spam'], vec, 30)\n",
    "\n",
    "top_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3 Creating Features\n",
    "\n",
    "Take the top-30 spam + top-30 ham words, and create a new CountVectorizer,\n",
    "called *relevant_vec*, which _only_ includes those words.\n",
    "See http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Vector of 'important' words\n",
    "top_ham = top_k(sms_df[sms_df['class'] == 'ham'], vec, 30)\n",
    "vocab = top_ham.append(top_spam)\n",
    "\n",
    "relevant_vec = CountVectorizer(decode_error = 'ignore', stop_words = 'english', vocabulary=vocab.index.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.09110867],\n",
       "       [ 3.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.16684962],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.05049396],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.04939627],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.02854007],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.03841932]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.model_selection as ms\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "# X is the feature array, based off relevant words\n",
    "X = relevant_vec.fit_transform(sms_df['sms'].values).toarray()\n",
    "\n",
    "# Compute the length of each sms message, normalized\n",
    "# by max length\n",
    "Xlen = np.zeros((X.shape[0],1))\n",
    "inx = 0\n",
    "for v in sms_df['sms'].values:\n",
    "        Xlen[inx,0] = len(v)\n",
    "        inx += 1\n",
    "Xlen = Xlen / max(Xlen)\n",
    "# Add the length as another feature\n",
    "X = np.hstack((X, Xlen))\n",
    "\n",
    "y = np.array((sms_df['class'] == 'spam').astype(int))\n",
    "\n",
    "# Now we split...\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, \n",
    "                                                    y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4 Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Results, as a list of dictionaries\n",
    "classifier_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Code for creating and testing classifiers mentioned in Step 1.4 of HW document\n",
    "for depth in range(1, 6):\n",
    "    dt_model = DecisionTreeClassifier(max_depth=depth)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    y_pred_test = dt_model.predict(X_test)\n",
    "    test_score = dt_model.score(X_test, y_test)\n",
    "\n",
    "    classifier_results.append({'Classifier': 'DecTree', 'Depth': depth, 'Score': test_score})\n",
    "    \n",
    "\n",
    "log1 = LogisticRegression(solver='liblinear', penalty='l1', random_state=42).fit(X_train, y_train)\n",
    "y_pred_test = log1.predict(X_test)\n",
    "test_score = log1.score(X_test, y_test)\n",
    "classifier_results.append({'Classifier': 'LogReg-L1', 'Score': test_score})\n",
    "\n",
    "log2 = LogisticRegression(solver='liblinear', penalty='l2', random_state=42).fit(X_train, y_train)\n",
    "y_pred_test = log2.predict(X_test)\n",
    "test_score = log2.score(X_test, y_test)\n",
    "classifier_results.append({'Classifier': 'LogReg-L2', 'Score': test_score})\n",
    "\n",
    "svm = SVC(random_state=42).fit(X_train, y_train)\n",
    "y_pred_test = svm.predict(X_test)\n",
    "test_score = svm.score(X_test, y_test)\n",
    "classifier_results.append({'Classifier': 'SVM', 'Score': test_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.939910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.947085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.950673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.961435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogReg-L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogReg-L2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classifier  Depth     Score\n",
       "0    DecTree    1.0  0.939910\n",
       "1    DecTree    2.0  0.939910\n",
       "2    DecTree    3.0  0.947085\n",
       "3    DecTree    4.0  0.950673\n",
       "4    DecTree    5.0  0.961435\n",
       "5  LogReg-L1    NaN  0.971300\n",
       "6  LogReg-L2    NaN  0.970404\n",
       "7        SVM    NaN  0.971300"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classifier_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 2.0 Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ensemble classifier results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Code for classifier construction and testing mentioned in Step 2.0 of HW document\n",
    "classifier_names = ['RandomForest', 'Bag-DecTree', 'Bag-LogReg-L1', 'Bag-LogReg-L2', 'Bag-SVM', \\\n",
    "                    'Boost-DecTree', 'Boost-LogReg-L1', 'Boost-LogReg-L2', 'Boost-SVM']\n",
    "rmf = RandomForestClassifier(n_estimators=31, random_state=314)\n",
    "dtb1 = BaggingClassifier(DecisionTreeClassifier(), random_state=42, n_estimators=31)\n",
    "lgb1 = BaggingClassifier(LogisticRegression(solver='liblinear', penalty='l1', random_state=42), n_estimators=31, random_state=314)\n",
    "lgb2 = BaggingClassifier(LogisticRegression(solver='liblinear', penalty='l2', random_state=42), n_estimators=31, random_state=314)\n",
    "svmBag = BaggingClassifier(SVC(random_state=42),n_estimators=31, random_state=314)\n",
    "\n",
    "dtada1 = AdaBoostClassifier(DecisionTreeClassifier(), random_state=42, n_estimators=31)\n",
    "lgada1 = AdaBoostClassifier(LogisticRegression(solver='liblinear', penalty='l1', random_state=42), n_estimators=31, random_state=314)\n",
    "lgada2 = AdaBoostClassifier(LogisticRegression(solver='liblinear', penalty='l2', random_state=42), n_estimators=31, random_state=314)\n",
    "svmada = AdaBoostClassifier(SVC(random_state=42), algorithm='SAMME', n_estimators=31, random_state=314)\n",
    "\n",
    "\n",
    "classifiers = [rmf, dtb1, lgb1, lgb2, svmBag, dtada1, lgada1, lgada2, svmada]\n",
    "\n",
    "\n",
    "pbar = tqdm(enumerate(classifiers), total=9)\n",
    "for i, classifier in pbar:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    test_score = classifier.score(X_test, y_test)\n",
    "    \n",
    "    classifier_results.append({'Classifier': classifier_names[i], 'Score': test_score, 'Count': len(classifier.estimators_)})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.0 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Count</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.939910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.947085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.950673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.961435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogReg-L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogReg-L2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag-DecTree</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bag-LogReg-L1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bag-LogReg-L2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bag-SVM</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Boost-DecTree</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boost-LogReg-L1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Boost-LogReg-L2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Boost-SVM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier  Count  Depth     Score\n",
       "0           DecTree    NaN    1.0  0.939910\n",
       "1           DecTree    NaN    2.0  0.939910\n",
       "2           DecTree    NaN    3.0  0.947085\n",
       "3           DecTree    NaN    4.0  0.950673\n",
       "4           DecTree    NaN    5.0  0.961435\n",
       "5         LogReg-L1    NaN    NaN  0.971300\n",
       "6         LogReg-L2    NaN    NaN  0.970404\n",
       "7               SVM    NaN    NaN  0.971300\n",
       "8      RandomForest   31.0    NaN  0.982960\n",
       "9       Bag-DecTree   31.0    NaN  0.980269\n",
       "10    Bag-LogReg-L1   31.0    NaN  0.973991\n",
       "11    Bag-LogReg-L2   31.0    NaN  0.971300\n",
       "12          Bag-SVM   31.0    NaN  0.972197\n",
       "13    Boost-DecTree   31.0    NaN  0.969507\n",
       "14  Boost-LogReg-L1   31.0    NaN  0.865471\n",
       "15  Boost-LogReg-L2   31.0    NaN  0.950673\n",
       "16        Boost-SVM    2.0    NaN  0.865471"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classifier_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 3.0 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Braden/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Code for classifier construction and testing mentioned in Step 3.0 of HW document\n",
    "\n",
    "perceptron = Perceptron(random_state=42)\n",
    "MLP1 = MLPClassifier((3), random_state=42)\n",
    "MLP2 = MLPClassifier((10), random_state=42)\n",
    "MLP3 = MLPClassifier((10, 10, 10), random_state=42)\n",
    "\n",
    "classifiers = [perceptron, MLP1, MLP2, MLP3]\n",
    "classifier_names = ['Perceptron', 'MLPClassifier', 'MLPClassifier', 'MLPClassifier']\n",
    "layers = [None, '(3,)', '(10,)', '(10, 10, 10)']\n",
    "\n",
    "test = []\n",
    "\n",
    "pbar = tqdm(enumerate(classifiers), total=4)\n",
    "for i, classifier in pbar:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    test_score = classifier.score(X_test, y_test)\n",
    "    if ('MLP' in classifier_names[i]):\n",
    "        classifier_results.append({'Classifier': classifier_names[i], 'Score': test_score, 'Hidden': layers[i]})\n",
    "    else:\n",
    "        classifier_results.append({'Classifier': classifier_names[i], 'Score': test_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Count</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Hidden</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.939910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.947085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.961435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogReg-L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogReg-L2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bag-DecTree</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bag-LogReg-L1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bag-LogReg-L2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bag-SVM</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Boost-DecTree</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.969507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boost-LogReg-L1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Boost-LogReg-L2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Boost-SVM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>0.977578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.972197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(10, 10, 10)</td>\n",
       "      <td>0.973094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier  Count  Depth        Hidden     Score\n",
       "0           DecTree    NaN    1.0           NaN  0.939910\n",
       "1           DecTree    NaN    2.0           NaN  0.939910\n",
       "2           DecTree    NaN    3.0           NaN  0.947085\n",
       "3           DecTree    NaN    4.0           NaN  0.950673\n",
       "4           DecTree    NaN    5.0           NaN  0.961435\n",
       "5         LogReg-L1    NaN    NaN           NaN  0.971300\n",
       "6         LogReg-L2    NaN    NaN           NaN  0.970404\n",
       "7               SVM    NaN    NaN           NaN  0.971300\n",
       "8      RandomForest   31.0    NaN           NaN  0.982960\n",
       "9       Bag-DecTree   31.0    NaN           NaN  0.980269\n",
       "10    Bag-LogReg-L1   31.0    NaN           NaN  0.973991\n",
       "11    Bag-LogReg-L2   31.0    NaN           NaN  0.971300\n",
       "12          Bag-SVM   31.0    NaN           NaN  0.972197\n",
       "13    Boost-DecTree   31.0    NaN           NaN  0.969507\n",
       "14  Boost-LogReg-L1   31.0    NaN           NaN  0.865471\n",
       "15  Boost-LogReg-L2   31.0    NaN           NaN  0.950673\n",
       "16        Boost-SVM    2.0    NaN           NaN  0.865471\n",
       "17       Perceptron    NaN    NaN           NaN  0.956054\n",
       "18    MLPClassifier    NaN    NaN          (3,)  0.977578\n",
       "19    MLPClassifier    NaN    NaN         (10,)  0.972197\n",
       "20    MLPClassifier    NaN    NaN  (10, 10, 10)  0.973094"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classifier_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.0 TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.7.0-cp36-cp36m-macosx_10_11_x86_64.whl (45.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 45.3MB 15kB/s  eta 0:00:01   22% |███████▎                        | 10.2MB 4.0MB/s eta 0:00:09\n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading absl-py-0.1.13.tar.gz (80kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/Braden/anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/Braden/anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting protobuf>=3.4.0 (from tensorflow)\n",
      "  Downloading protobuf-3.5.2.post1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 455kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading gast-0.2.0.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading astor-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/Braden/anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting tensorboard<1.8.0,>=1.7.0 (from tensorflow)\n",
      "  Downloading tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 212kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading grpcio-1.10.1-cp36-cp36m-macosx_10_11_x86_64.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 376kB/s ta 0:00:011    91% |█████████████████████████████▏  | 1.4MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/Braden/anaconda/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Requirement already satisfied: bleach==1.5.0 in /Users/Braden/anaconda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 543kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /Users/Braden/anaconda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/Braden/anaconda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow)\n",
      "Building wheels for collected packages: absl-py, termcolor, gast, html5lib\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/Braden/Library/Caches/pip/wheels/76/f7/0c/88796d7212af59bb2f496b12267e0605f205170781e9b86479\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/Braden/Library/Caches/pip/wheels/de/f7/bf/1bcac7bf30549e6a4957382e2ecab04c88e513117207067b03\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/Braden/Library/Caches/pip/wheels/8e/fa/d6/77dd17d18ea23fd7b860e02623d27c1be451521af40dd4a13e\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/Braden/Library/Caches/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built absl-py termcolor gast html5lib\n",
      "Installing collected packages: absl-py, termcolor, protobuf, gast, astor, html5lib, tensorboard, grpcio, tensorflow\n",
      "  Found existing installation: html5lib 0.999\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (html5lib) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling html5lib-0.999:\n",
      "      Successfully uninstalled html5lib-0.999\n",
      "Successfully installed absl-py-0.1.13 astor-0.6.2 gast-0.2.0 grpcio-1.10.1 html5lib-0.9999999 protobuf-3.5.2.post1 tensorboard-1.7.0 tensorflow-1.7.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Define TensorFlow columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = vocab.index.drop_duplicates().values.tolist()\n",
    "columns.extend(['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Create function input_fn(x,y)\n",
    "def input_fn(x, y):\n",
    "    tensors = {}\n",
    "    for i in range(len(x[0])):\n",
    "        key = columns[i]\n",
    "        tensors[key] = tf.convert_to_tensor(x[:, i])\n",
    "    \n",
    "    return tensors, tf.convert_to_tensor(y)\n",
    "                         \n",
    "# TODO: Create function train_input_fn()\n",
    "def train_input_fn():\n",
    "    x_tensors, y_tensor = input_fn(X_train, y_train)\n",
    "    return x_tensors, y_tensor\n",
    "\n",
    "# TODO: Create function test_input_fn()\n",
    "def test_input_fn():\n",
    "    x_tensors, y_tensor = input_fn(X_test, y_test)\n",
    "    return x_tensors, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpue42smfl\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpue42smfl', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a289bd9b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create DNNClassifier\n",
    "\n",
    "features = []\n",
    "\n",
    "for col in columns:\n",
    "    features.append(tf.feature_column.numeric_column(col))\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "estimator = tf.estimator.DNNClassifier(hidden_units=[5, 5], feature_columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3.1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpue42smfl/model.ckpt.\n",
      "INFO:tensorflow:loss = 3256.69, step = 1\n",
      "INFO:tensorflow:global_step/sec: 216.707\n",
      "INFO:tensorflow:loss = 275.016, step = 101 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.401\n",
      "INFO:tensorflow:loss = 247.156, step = 201 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.105\n",
      "INFO:tensorflow:loss = 226.104, step = 301 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.392\n",
      "INFO:tensorflow:loss = 212.831, step = 401 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 325.508\n",
      "INFO:tensorflow:loss = 203.286, step = 501 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.74\n",
      "INFO:tensorflow:loss = 195.916, step = 601 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.194\n",
      "INFO:tensorflow:loss = 189.828, step = 701 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.153\n",
      "INFO:tensorflow:loss = 184.999, step = 801 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.616\n",
      "INFO:tensorflow:loss = 181.659, step = 901 (0.549 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpue42smfl/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 179.031.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1a28a54da0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: train\n",
    "estimator.train(input_fn=train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-08-23:31:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpue42smfl/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Evaluation [200/1000]\n",
      "INFO:tensorflow:Evaluation [300/1000]\n",
      "INFO:tensorflow:Evaluation [400/1000]\n",
      "INFO:tensorflow:Evaluation [500/1000]\n",
      "INFO:tensorflow:Evaluation [600/1000]\n",
      "INFO:tensorflow:Evaluation [700/1000]\n",
      "INFO:tensorflow:Evaluation [800/1000]\n",
      "INFO:tensorflow:Evaluation [900/1000]\n",
      "INFO:tensorflow:Evaluation [1000/1000]\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-08-23:31:34\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.976682, accuracy_baseline = 0.865471, auc = 0.973534, auc_precision_recall = 0.946025, average_loss = 0.118247, global_step = 1000, label/mean = 0.134529, loss = 131.845, prediction/mean = 0.130101\n"
     ]
    }
   ],
   "source": [
    "# TODO: evaluate\n",
    "eval_results = estimator.evaluate(test_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.97668159,\n",
       " 'accuracy_baseline': 0.86547089,\n",
       " 'auc': 0.97353369,\n",
       " 'auc_precision_recall': 0.94602466,\n",
       " 'average_loss': 0.11824686,\n",
       " 'global_step': 1000,\n",
       " 'label/mean': 0.13452914,\n",
       " 'loss': 131.84525,\n",
       " 'prediction/mean': 0.13010089}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: results\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpf59009kw\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpf59009kw', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a2930f8d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create LinearClassifier\n",
    "estimator = tf.estimator.LinearClassifier(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpf59009kw/model.ckpt.\n",
      "INFO:tensorflow:loss = 3089.35, step = 1\n",
      "INFO:tensorflow:global_step/sec: 130.88\n",
      "INFO:tensorflow:loss = 476.177, step = 101 (0.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 325.374\n",
      "INFO:tensorflow:loss = 388.482, step = 201 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.974\n",
      "INFO:tensorflow:loss = 355.363, step = 301 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 326.319\n",
      "INFO:tensorflow:loss = 338.255, step = 401 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.881\n",
      "INFO:tensorflow:loss = 327.982, step = 501 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.302\n",
      "INFO:tensorflow:loss = 321.222, step = 601 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.764\n",
      "INFO:tensorflow:loss = 316.484, step = 701 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.679\n",
      "INFO:tensorflow:loss = 313.004, step = 801 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.637\n",
      "INFO:tensorflow:loss = 310.353, step = 901 (0.320 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpf59009kw/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 308.292.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x1a2930fef0>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: train\n",
    "estimator.train(input_fn=train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-08-23:48:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/5d/_d8b4hsx08b9hpgyqpqmvnjr0000gn/T/tmpf59009kw/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Evaluation [200/1000]\n",
      "INFO:tensorflow:Evaluation [300/1000]\n",
      "INFO:tensorflow:Evaluation [400/1000]\n",
      "INFO:tensorflow:Evaluation [500/1000]\n",
      "INFO:tensorflow:Evaluation [600/1000]\n",
      "INFO:tensorflow:Evaluation [700/1000]\n",
      "INFO:tensorflow:Evaluation [800/1000]\n",
      "INFO:tensorflow:Evaluation [900/1000]\n",
      "INFO:tensorflow:Evaluation [1000/1000]\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-08-23:48:30\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.974888, accuracy_baseline = 0.865471, auc = 0.976957, auc_precision_recall = 0.945623, average_loss = 0.096756, global_step = 1000, label/mean = 0.134529, loss = 107.883, prediction/mean = 0.13451\n"
     ]
    }
   ],
   "source": [
    "# TODO: evaluate\n",
    "eval_results = estimator.evaluate(test_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.97488791,\n",
       " 'accuracy_baseline': 0.86547089,\n",
       " 'auc': 0.97695678,\n",
       " 'auc_precision_recall': 0.94562262,\n",
       " 'average_loss': 0.096755996,\n",
       " 'global_step': 1000,\n",
       " 'label/mean': 0.13452914,\n",
       " 'loss': 107.88293,\n",
       " 'prediction/mean': 0.13451025}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: results\n",
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "188275d9f0da4528abed186d84317274": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "4ab9c93fd8dd4aeeb71119aecc43395e": {
     "views": [
      {
       "cell_index": 36
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
